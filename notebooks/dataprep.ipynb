{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7437</td><td>application_1513605045578_4983</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_4983/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop32:8042/node/containerlogs/container_e28_1513605045578_4983_01_000001/ID2223nyctaxi__stornq00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define some variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import geohash2 as gh2\n",
    "\n",
    "#Global variables\n",
    "g = 7 #geohash length\n",
    "b = 48 # number of time bins per day\n",
    "# Note: b must evenly divide 60\n",
    "minutes_per_bin = int((24 / float(b)) * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Extract and reformat the data\n",
    "\n",
    "##### geohash\n",
    "A categorical representation of (longitude, latitude) used as an id and dropped before prediction.\n",
    "##### time_cat\n",
    "A categorical representation of the time of day, number of categories is controlled by $ b $. ($b = 48$ means every half hour). The value of the bin is the centerpoint in the timespan.\n",
    "##### time_num\n",
    "A float representation of the time of day (as bins) between 0 and 1. The center of the bin is converted to a floating point number, e.g. 20:30 is converted to $41/48 = 0.85416666666$ \n",
    "#### time_cos\n",
    "The binned time variable time_num is converted to a cosine value to even out the transition between days.\n",
    "#### time_sin\n",
    "The same purpose only using the sine function instead.\n",
    "#### day_num\n",
    "Day of week as a binned feature between 0 (Monday morning) to 1 (Sunday night).\n",
    "#### day_cos\n",
    "Binned day_num in a cosine representation.\n",
    "#### day_sin\n",
    "Binned day_num in a sine representation.\n",
    "#### weekend\n",
    "0 if weekday, 1 if weekend.\n",
    "#### Location features\n",
    "The geohashed value of the pickup and dropoff location is returned and also the the longitude and latitude of the pickup location (decoded from the geohashed value for loss of precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def date_extractor(date_str,b,minutes_per_bin):\n",
    "    # Takes a datetime object as a parameter\n",
    "    # and extracts and returns a tuple of the form: (as per the data specification)\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)\n",
    "    # Split date string into list of date, time\n",
    "    \n",
    "    d = date_str.split()\n",
    "    \n",
    "    #safety check\n",
    "    if len(d) != 2:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # TIME (eg. for 16:56:20 and 15 mins per bin)\n",
    "    #list of hour,min,sec (e.g. [16,56,20])\n",
    "    time_list = [int(t) for t in d[1].split(':')]\n",
    "    \n",
    "    #safety check\n",
    "    if len(time_list) != 3:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # calculate number of minute into the day (eg. 1016)\n",
    "    num_minutes = time_list[0] * 60 + time_list[1]\n",
    "    \n",
    "    # Time of the start of the bin\n",
    "    time_bin = num_minutes / minutes_per_bin     # eg. 1005\n",
    "    hour_bin = num_minutes / 60                  # eg. 16\n",
    "    min_bin = (time_bin * minutes_per_bin) % 60  # eg. 45\n",
    "    \n",
    "    #get time_cat\n",
    "    hour_str = str(hour_bin) if hour_bin / 10 > 0 else \"0\" + str(hour_bin)  # eg. \"16\"\n",
    "    min_str = str(min_bin) if min_bin / 10 > 0 else \"0\" + str(min_bin)      # eg. \"45\"\n",
    "    time_cat = hour_str + \":\" + min_str                                     # eg. \"16:45\"\n",
    "    \n",
    "    # Get a floating point representation of the center of the time bin\n",
    "    time_num = (hour_bin*60 + min_bin + minutes_per_bin / 2.0)/(60*24)      # eg. 0.7065972222222222\n",
    "    \n",
    "    time_cos = math.cos(time_num * 2 * math.pi)\n",
    "    time_sin = math.sin(time_num * 2 * math.pi)\n",
    "    \n",
    "    # DATE\n",
    "    # Parse year, month, day\n",
    "    date_list = d[0].split('-')\n",
    "    d_obj = date(int(date_list[0]),int(date_list[1]),int(date_list[2]))\n",
    "    day_to_str = {0: \"Monday\",\n",
    "                  1: \"Tuesday\",\n",
    "                  2: \"Wednesday\",\n",
    "                  3: \"Thursday\",\n",
    "                  4: \"Friday\",\n",
    "                  5: \"Saturday\",\n",
    "                  6: \"Sunday\"}\n",
    "    day_of_week = d_obj.weekday()\n",
    "    day_cat = day_to_str[day_of_week]\n",
    "    day_num = (day_of_week + time_num)/7.0\n",
    "    day_cos = math.cos(day_num * 2 * math.pi)\n",
    "    day_sin = math.sin(day_num * 2 * math.pi)\n",
    "    \n",
    "    year = d_obj.year\n",
    "    month = d_obj.month\n",
    "    day = d_obj.day\n",
    "    \n",
    "    weekend = 0\n",
    "    #check if it is the weekend\n",
    "    if day_of_week in [5,6]:\n",
    "        weekend = 1\n",
    "       \n",
    "    return (year, month, day, time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define two different data cleaning methods as the data is structured slightly differently for the yellow and green data. For this project the main cause was the placement of the dropoff longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def yellow_data_cleaner(row):\n",
    "    # takes a tuple (row,g,b,minutes_per_bin) as a parameter and returns a tuple of the form:\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend,geohash)\n",
    "    \n",
    "    #safety check: make sure row has enough features\n",
    "    if len(row) < 7:\n",
    "        return None\n",
    "    \n",
    "    #extract day of the week and hour\n",
    "    date_str = row[1]\n",
    "    clean_date = date_extractor(date_str,b,minutes_per_bin)\n",
    "    \n",
    "    # beware the order\n",
    "    pickup_longitude = float(row[5])\n",
    "    pickup_latitude = float(row[6])\n",
    "    \n",
    "    #get geo hashed pickup and dropoff locatation\n",
    "    pickup_location = gh2.encode(pickup_latitude, pickup_longitude)\n",
    "    #pickup_location = (pickup_latitude, pickup_longitude)\n",
    "    #label for task 3 \n",
    "    #dropoff_location = gh2.encode(float(row[9]), float(row[10]))\n",
    "    #dropoff_location = (row[7], row[8]) \n",
    "    (decoded_lat, decoded_long) = gh2.decode(pickup_location)\n",
    "\n",
    "    #safety check: make sure latitude and longitude are valid, i.e. inside NYC\n",
    "    if pickup_latitude < 41.1 and pickup_latitude > 40.5 and pickup_longitude < -73.6 and pickup_longitude > -74.1:\n",
    "        return tuple(list(clean_date)+[pickup_location]+[decoded_lat]+[decoded_long])\n",
    "        #note: removed dropoff_location from the return values\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def green_data_cleaner(row):\n",
    "    # takes a tuple (row,g,b,minutes_per_bin) as a parameter and returns a tuple of the form:\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend,geohash)\n",
    "    \n",
    "    #safety check: make sure row has enough features\n",
    "    if len(row) < 7:\n",
    "        return None\n",
    "    \n",
    "    #extract day of the week and hour\n",
    "    date_str = row[1]\n",
    "    clean_date = date_extractor(date_str,b,minutes_per_bin)\n",
    "    \n",
    "    # beware the order\n",
    "    pickup_longitude = float(row[5])\n",
    "    pickup_latitude = float(row[6])\n",
    "    \n",
    "    #get geo hashed pickup and dropoff locatation\n",
    "    pickup_location = gh2.encode(pickup_latitude, pickup_longitude)\n",
    "    #pickup_location = (pickup_latitude, pickup_longitude)\n",
    "    #label for task 3 \n",
    "    #dropoff_location = gh2.encode(float(row[7]), float(row[8]))\n",
    "    #dropoff_location = (row[7], row[8]) \n",
    "    (decoded_lat, decoded_long) = gh2.decode(pickup_location)\n",
    "\n",
    "    #safety check: make sure latitude and longitude are valid, i.e. inside NYC\n",
    "    if pickup_latitude < 41.1 and pickup_latitude > 40.5 and pickup_longitude < -73.6 and pickup_longitude > -74.1:\n",
    "        return tuple(list(clean_date)+[pickup_location]+[decoded_lat]+[decoded_long])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define function for reading the weather data into a DataFrame (pandas) and for preparing data \n",
    "\n",
    "We read the weather from a measurement station in Central Park in New York City. It contains data regarding precipitation, snowfall, snow depth, wind speed and minimum and maximum temperatures. From that we extract the year, month and day from the intial data. As for temperature it is converted from Fahrenheit to Celsius.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def weather_extractor():\n",
    "    raw_weather = spark.read.load(\"hdfs:///Projects/labs/nyc_taxi_data/data/central_park_weather.csv\", format='com.databricks.spark.csv', header='true',inferSchema='true')\n",
    "    weather_df = raw_weather.drop(\"STATION\", \"STATION_NAME\").toPandas()\n",
    "    \n",
    "    weather_df[\"year\"] = (weather_df[\"DATE\"]/10000).apply(math.floor).astype(int)\n",
    "    weather_df[\"month\"] = (weather_df[\"DATE\"].mod(10000)/100).apply(math.floor).astype(int)\n",
    "    weather_df[\"day\"] = weather_df[\"DATE\"].mod(100)\n",
    "    \n",
    "    weather_df[\"PRCP\"] = weather_df[\"PRCP\"]/10\n",
    "    weather_df[\"SNWD\"] = weather_df[\"SNWD\"]/10\n",
    "    weather_df[\"SNOW\"] = weather_df[\"SNOW\"]/10\n",
    "    weather_df[\"AWND\"] = weather_df[\"AWND\"]/10*3.6\n",
    "    \n",
    "    weather_df[\"TMAX\"] = (weather_df[\"TMAX\"]-32)/1.8\n",
    "    weather_df[\"TMIN\"] = (weather_df[\"TMIN\"]-32)/1.8\n",
    "    \n",
    "    weather_df1 = weather_df.drop(\"DATE\", 1)\n",
    "    return weather_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the number of the day in the year. So January 1st is 1, December 31st is 365 or 366\n",
    "def get_yearday(df):\n",
    "    date = datetime.date(df['year'],df['month'],df['day'])\n",
    "    return (date.timetuple().tm_yday-1)/365."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "Read the traffic data, remove abnormal entries and clean it. Then count the number of pickups for each location and append it as a new column. Thereafter, clean the weather data and merge the two DataFrames into one which is persisted to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graw = spark.read.format(\"CSV\").option(\"header\",\"true\").load(\"hdfs:///Projects/labs/nyc_taxi_data/data/green_tripdata*\")\n",
    "yraw = spark.read.format(\"CSV\").option(\"header\",\"true\").load(\"hdfs:///Projects/labs/nyc_taxi_data/data/yellow_tripdata*\")\n",
    "\n",
    "#print(graw.count())\n",
    "#print(yraw.count())\n",
    "\n",
    "gclean_rdd = graw.rdd.map(green_data_cleaner).filter(lambda row: row != None)\n",
    "yclean_rdd = yraw.rdd.map(yellow_data_cleaner).filter(lambda row: row != None)\n",
    "\n",
    "gclean_extracol = gclean_rdd.map(lambda row: row + (1,))\n",
    "yclean_extracol = yclean_rdd.map(lambda row: row + (1,))\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schemaString = \"year month day time_cat time_num time_cos time_sin day_cat day_num day_cos day_sin weekend pickup_location pickup_latitude pickup_longitude count\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "ginit_df = spark.createDataFrame(gclean_extracol, schema)\n",
    "yinit_df = spark.createDataFrame(yclean_extracol, schema)\n",
    "\n",
    "ygdf = ginit_df.unionAll(yinit_df)\n",
    "#print(ygdf.count())\n",
    "ygdf_summed = ygdf.groupBy(\"pickup_location\").agg({\"count\": \"sum\"})\n",
    "#ygdf_summed = ginit_df.join(yinit_df, [\"year\", \"month\", \"day\", \"time_cat\", \"time_num\", \"time_cos\", \"time_sin\", \"day_cat\", \"day_num\", \"day_cos\", \"day_sin\", \"weekend\", \"pickup_location\", \"pickup_latitude\", \"pickup_longitude\", \"count\"], \"full_outer\") \\\n",
    "#.groupBy(\"pickup_location\").agg({\"count\": \"sum\"})\n",
    "\n",
    "ygdf_done = ygdf.join(ygdf_summed, \"pickup_location\").drop(\"count\", \"pickup_location\").withColumnRenamed(\"sum(count)\", \"label\")\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "weather_pddf = weather_extractor()\n",
    "sqlContext = SQLContext(spark)\n",
    "weather_df = sqlContext.createDataFrame(weather_pddf)\n",
    "#print(ygdf_done.rdd.count())\n",
    "#print(weather_df.rdd.count())\n",
    "df_merged = ygdf_done.join(weather_df, [\"year\", \"month\", \"day\"])\n",
    "#print(df_merged.rdd.count())\n",
    "\n",
    "df_merged.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///Projects/ID2223nyctaxi/prepared_taxirides/yellow_and_green_data_finally2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
