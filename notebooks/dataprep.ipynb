{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import geohash2 as gh2\n",
    "\n",
    "#Global variables\n",
    "g = 7 #geohash length\n",
    "b = 48 # number of time bins per day\n",
    "# Note: b must evenly divide 60\n",
    "minutes_per_bin = int((24 / float(b)) * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def date_extractor(date_str,b,minutes_per_bin):\n",
    "    # Takes a datetime object as a parameter\n",
    "    # and extracts and returns a tuple of the form: (as per the data specification)\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)\n",
    "    # Split date string into list of date, time\n",
    "    \n",
    "    d = date_str.split()\n",
    "    \n",
    "    #safety check\n",
    "    if len(d) != 2:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # TIME (eg. for 16:56:20 and 15 mins per bin)\n",
    "    #list of hour,min,sec (e.g. [16,56,20])\n",
    "    time_list = [int(t) for t in d[1].split(':')]\n",
    "    \n",
    "    #safety check\n",
    "    if len(time_list) != 3:\n",
    "        return tuple([None,])\n",
    "    \n",
    "    # calculate number of minute into the day (eg. 1016)\n",
    "    num_minutes = time_list[0] * 60 + time_list[1]\n",
    "    \n",
    "    # Time of the start of the bin\n",
    "    time_bin = num_minutes / minutes_per_bin     # eg. 1005\n",
    "    hour_bin = num_minutes / 60                  # eg. 16\n",
    "    min_bin = (time_bin * minutes_per_bin) % 60  # eg. 45\n",
    "    \n",
    "    #get time_cat\n",
    "    hour_str = str(hour_bin) if hour_bin / 10 > 0 else \"0\" + str(hour_bin)  # eg. \"16\"\n",
    "    min_str = str(min_bin) if min_bin / 10 > 0 else \"0\" + str(min_bin)      # eg. \"45\"\n",
    "    time_cat = hour_str + \":\" + min_str                                     # eg. \"16:45\"\n",
    "    \n",
    "    # Get a floating point representation of the center of the time bin\n",
    "    time_num = (hour_bin*60 + min_bin + minutes_per_bin / 2.0)/(60*24)      # eg. 0.7065972222222222\n",
    "    \n",
    "    time_cos = math.cos(time_num * 2 * math.pi)\n",
    "    time_sin = math.sin(time_num * 2 * math.pi)\n",
    "    \n",
    "    # DATE\n",
    "    # Parse year, month, day\n",
    "    date_list = d[0].split('-')\n",
    "    d_obj = date(int(date_list[0]),int(date_list[1]),int(date_list[2]))\n",
    "    day_to_str = {0: \"Monday\",\n",
    "                  1: \"Tuesday\",\n",
    "                  2: \"Wednesday\",\n",
    "                  3: \"Thursday\",\n",
    "                  4: \"Friday\",\n",
    "                  5: \"Saturday\",\n",
    "                  6: \"Sunday\"}\n",
    "    day_of_week = d_obj.weekday()\n",
    "    day_cat = day_to_str[day_of_week]\n",
    "    day_num = (day_of_week + time_num)/7.0\n",
    "    day_cos = math.cos(day_num * 2 * math.pi)\n",
    "    day_sin = math.sin(day_num * 2 * math.pi)\n",
    "    \n",
    "    year = d_obj.year\n",
    "    month = d_obj.month\n",
    "    day = d_obj.day\n",
    "    \n",
    "    weekend = 0\n",
    "    #check if it is the weekend\n",
    "    if day_of_week in [5,6]:\n",
    "        weekend = 1\n",
    "       \n",
    "    return (year, month, day, time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend)\n",
    "\n",
    "def data_cleaner(row):\n",
    "    # takes a tuple (row,g,b,minutes_per_bin) as a parameter and returns a tuple of the form:\n",
    "    # (time_cat, time_num, time_cos, time_sin, day_cat, day_num, day_cos, day_sin, weekend,geohash)\n",
    "    \n",
    "    #safety check: make sure row has enough features\n",
    "    if len(row) < 7:\n",
    "        return None\n",
    "    \n",
    "    #extract day of the week and hour\n",
    "    date_str = row[1]\n",
    "    clean_date = date_extractor(date_str,b,minutes_per_bin)\n",
    "    \n",
    "    # beware the order\n",
    "    pickup_longitude = float(row[5])\n",
    "    pickup_latitude = float(row[6])\n",
    "    \n",
    "    #get geo hashed pickup and dropoff locatation\n",
    "    pickup_location = gh2.encode(pickup_latitude, pickup_longitude)\n",
    "    #pickup_location = (pickup_latitude, pickup_longitude)\n",
    "    #label for task 3 \n",
    "    dropoff_location = gh2.encode(float(row[7]), float(row[8]))\n",
    "    #dropoff_location = (row[7], row[8]) \n",
    "    \n",
    "    #safety check: make sure latitude and longitude are valid, i.e. inside NYC\n",
    "    if pickup_latitude < 41.1 and pickup_latitude > 40.5 and pickup_longitude < -73.6 and pickup_longitude > -74.1:\n",
    "        return tuple(list(clean_date)+[pickup_location]+[dropoff_location]+[pickup_latitude]+[pickup_longitude])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+--------+-------------------+--------------------+-------+-------+-------------------+------------------+-------+-----------------+-----------------+-----+\n",
      "|year|month|day|time_cat|time_num|           time_cos|            time_sin|day_cat|day_num|            day_cos|           day_sin|weekend|  pickup_latitude| pickup_longitude|label|\n",
      "+----+-----+---+--------+--------+-------------------+--------------------+-------+-------+-------------------+------------------+-------+-----------------+-----------------+-----+\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "|2013|   10| 22|   12:30| 0.53125|-0.9807852804032304|-0.19509032201612836|Tuesday|0.21875|0.19509032201612833|0.9807852804032304|      0|40.75799560546875|-73.9372787475586|147.0|\n",
      "+----+-----+---+--------+--------+-------------------+--------------------+-------+-------+-------------------+------------------+-------+-----------------+-----------------+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "raw_green = sc.textFile(\"hdfs:///Projects/labs/nyc_taxi_data/data/green_tripdata_2013-10.csv\")\n",
    "#raw_yellow = sc.textFile(\"yellow_tripdata_2017-06.csv\")\n",
    "\n",
    "headers = raw_green.first()\n",
    "\n",
    "raw2_green = raw_green.filter(lambda row : row != headers).map(lambda line: tuple(line.split(',')))\n",
    "\n",
    "#print(raw2_green.take(20))\n",
    "\n",
    "gclean_rdd = raw2_green.map(data_cleaner).filter(lambda row: row != None).cache()\n",
    "#print(gclean_rdd.take(20))\n",
    "\n",
    "gclean_extracol = gclean_rdd.map(lambda row: row + (1,))\n",
    "#print(gclean_extracol.take(20))\n",
    "\n",
    "#gclean_sum = gclean_extracol.groupBy(-2)\n",
    "#print(gclean_sum.take(20))\n",
    "#print(gclean_sum.take(20))\n",
    "\n",
    "#print(headers)\n",
    "\n",
    "#save to file\n",
    "#gclean_rdd.saveAsTextFile(\"hdfs:///Projects/demo_tensorflow_abarose0/Jupyter/small5\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schemaString = \"year month day time_cat time_num time_cos time_sin day_cat day_num day_cos day_sin weekend pickup_location dropoff_location pickup_latitude pickup_longitude count\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "init_df = spark.createDataFrame(gclean_extracol, schema)\n",
    "df_summed = init_df.groupBy(\"pickup_location\").agg({\"count\": \"sum\"})\n",
    "df_done = init_df.join(df_summed, \"pickup_location\").drop(\"count\", \"pickup_location\", \"dropoff_location\").withColumnRenamed(\"sum(count)\", \"label\")\n",
    "\n",
    "df_done.show(10)\n",
    "\n",
    "df_done.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///Projects/ID2223nyctaxi/prepared_taxirides/tiny_green_2013-10_no_geohash\")\n",
    "\n",
    "\n",
    "#other testing\n",
    "#pgh.decode('161')\n",
    "#pgh.encode(41,-74,7)\n",
    "#print(pgh.encode(40,74,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "drk14zg"
     ]
    }
   ],
   "source": [
    "#Test geocode2 package\n",
    "print(gh2.encode(41,-73,g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Other\n",
    "headers2 = [\"Year\", \"month\", \"day\", \"time_cat\", \"time_num\", \"time_cos\", \"time_sin\", \"day_cat\", \"day_num\", \"day_cos\", \"day_sin\", \"weekend\", \"Location\"]\n",
    "\n",
    "raw45 = sc.textFile(\"hdfs:///Projects/demo_tensorflow_abarose0/Jupyter/small2\")\n",
    "print(raw45.take(1))\n",
    "#raw46 = raw45.map(lambda x: (x, )).toDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
